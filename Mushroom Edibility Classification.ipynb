{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dimensional-external",
   "metadata": {},
   "source": [
    "# Mushroom Edibility Classification\n",
    "\n",
    "**Link**\n",
    "\n",
    "https://www.kaggle.com/uciml/mushroom-classification\n",
    "\n",
    "**Context**\n",
    "\n",
    "Although this dataset was originally contributed to the UCI Machine Learning repository nearly 30 years ago, mushroom hunting (otherwise known as \"shrooming\") is enjoying new peaks in popularity. Learn which features spell certain death and which are most palatable in this dataset of mushroom characteristics. And how certain can your model be?\n",
    "\n",
    "**Content**\n",
    "\n",
    "This dataset includes descriptions of hypothetical samples corresponding to 23 species of gilled mushrooms in the Agaricus and Lepiota Family Mushroom drawn from The Audubon Society Field Guide to North American Mushrooms (1981). Each species is identified through the target attribute `class` as definitely edible, definitely poisonous, or of unknown edibility and not recommended. This latter class was combined with the poisonous one.\n",
    "\n",
    "The dataset consists of the following attributes:\n",
    "- class - edible=e, poisonous=p\n",
    "- cap-shape: bell=b,conical=c,convex=x,flat=f, knobbed=k,sunken=s\n",
    "- cap-surface: fibrous=f,grooves=g,scaly=y,smooth=s\n",
    "- cap-color: brown=n,buff=b,cinnamon=c,gray=g,green=r,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "- bruises: bruises=t,no=f\n",
    "- odor: almond=a,anise=l,creosote=c,fishy=y,foul=f,musty=m,none=n,pungent=p,spicy=s\n",
    "- gill-attachment: attached=a,descending=d,free=f,notched=n\n",
    "- gill-spacing: close=c,crowded=w,distant=d\n",
    "- gill-size: broad=b,narrow=n\n",
    "- gill-color: black=k,brown=n,buff=b,chocolate=h,gray=g, green=r,orange=o,pink=p,purple=u,red=e,white=w,yellow=y\n",
    "- stalk-shape: enlarging=e,tapering=t\n",
    "- stalk-root: bulbous=b,club=c,cup=u,equal=e,rhizomorphs=z,rooted=r,missing=?\n",
    "- stalk-surface-above-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "- stalk-surface-below-ring: fibrous=f,scaly=y,silky=k,smooth=s\n",
    "- stalk-color-above-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "- stalk-color-below-ring: brown=n,buff=b,cinnamon=c,gray=g,orange=o,pink=p,red=e,white=w,yellow=y\n",
    "- veil-type: partial=p,universal=u\n",
    "- veil-color: brown=n,orange=o,white=w,yellow=y\n",
    "- ring-number: none=n,one=o,two=t\n",
    "- ring-type: cobwebby=c,evanescent=e,flaring=f,large=l,none=n,pendant=p,sheathing=s,zone=z\n",
    "- spore-print-color: black=k,brown=n,buff=b,chocolate=h,green=r,orange=o,purple=u,white=w,yellow=y\n",
    "- population: abundant=a,clustered=c,numerous=n,scattered=s,several=v,solitary=y\n",
    "- habitat: grasses=g,leaves=l,meadows=m,paths=p,urban=u,waste=w,woods=d\n",
    "\n",
    "**Task (Classification)**\n",
    "\n",
    "Your task is to use the present data set to predict the edibility of a mushroom sample. To do this, use the `Logistic Regression` and `XGBoost` methods for this task. You must also include a third method that you have selected yourself.\n",
    "\n",
    "First of all, get an overview of the project in your group. Then carefully read the checklist for machine learning projects and think about how you want to organize your group work. It is strongly recommended that all task items are completed by all group members. You can divide the focus among yourself, but make sure that all members are as well informed as possible about the content.\n",
    "\n",
    "Use the checklist for machine learning projects as a guide when working on the task. Document all the individual steps that are listed there (main and sub-items). Make sure to use Markdown Cells for your documentation. Document the functionality of your algorithms (all three) with equations and explanations. Dont forget, this project is a task for five students. We expect a detailed documentation of your approach and your results.\n",
    "\n",
    "**Note**\n",
    "\n",
    "We are aware that there are examples and solutions for the selected data sets on popular platforms, e.g. Kaggle. You are welcome to use them as a guide. But remember that at the end of the project, your own work will be assessed. We compare the results with the popular solutions of common platforms. We would like to recognize the independence in your work and see a difference to the existing solution approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "annual-estimate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb51306a",
   "metadata": {},
   "source": [
    "**ML Project Checklist**\n",
    "1. Frame the problem and look at the big picture. \n",
    "2. Get the data. \n",
    "3. Explore the data to gain insights. \n",
    "4. Prepare the data to better expose the underlying data patterns to Machine Learning algorithms. \n",
    "5. Explore many different models and short-list the best ones. \n",
    "6. Fine-tune your models and combine them into a great solution. \n",
    "7. Present your solution. \n",
    "8. Launch, monitor, and maintain your system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43601c5",
   "metadata": {},
   "source": [
    "**1. Framing the problem**\n",
    "\n",
    "General guidelines: \n",
    "- Define the objective in business terms.\n",
    "- How will your solution be used? \n",
    "- What are the current solutions/workarounds (if any)? \n",
    "- How should you frame this problem (supervised/unsupervised, online/offline, etc.) \n",
    "- How should performance be measured? \n",
    "- Is the performance measure aligned with the business objective? \n",
    "- What would be the minimum performance needed to reach the business objective? \n",
    "- What are comparable problems? Can you reuse experience or tools? \n",
    "- Is human expertise available? \n",
    "- How would you solve the problem manually? \n",
    "- List the assumptions you or others have made so far. \n",
    "- Verify assumptions if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb82b63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1e210fc2",
   "metadata": {},
   "source": [
    "**2. Getting the data**\n",
    "\n",
    "General guidelines: \n",
    "- List the data you need and how much you need. \n",
    "- Find and document where you can get that data. \n",
    "- Check how much space it will take. \n",
    "- Check legal obligations, and get the authorization if necessary. \n",
    "- Get access authorizations. \n",
    "- Create a workspace (with enough storage space). \n",
    "- Get the data. \n",
    "- Convert the data to a format you can easily manipulate (without changing the data itself). \n",
    "- Ensure sensitive information is deleted or protected (e.g., anonymized). \n",
    "- Check the size and type of data (time series, sample, geographical, etc.). \n",
    "- Sample a test set, put it aside, and never look at it (no data snooping!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1cf45c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mxgb\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import xgboost as xgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df=pd.read_csv('data/mushrooms.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae22c17",
   "metadata": {},
   "source": [
    "**3. Exploring the data**\n",
    "\n",
    "General guidelines:\n",
    "- Create a copy of the data for exploration (down sampling if necessary).\n",
    "- Keep record of your data exploration (Jupyter notebook).\n",
    "- Study each attribute and its characteristics: \n",
    "    Name, Type (categorical, int/float, bounded/unbounded, text, structured, etc.), % missing values, Noisiness (stochastic, outliers, rounding errors, etc.), \n",
    "    Type of distribution (Gaussian, uniform, logarithmic, etc.), Possibly useful for the task?\n",
    "- For supervised learning tasks, identify the target attribute(s).\n",
    "- Visualize the data. -> Scatterplot, HeatMaps etc\n",
    "- Study the correlations between attributes.\n",
    "- Study how you would solve the problem manually.\n",
    "- Identify the promising transformations you may want to apply.\n",
    "- Identify extra data that would be useful.\n",
    "- Document what you have learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50667f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8124 entries, 0 to 8123\n",
      "Data columns (total 23 columns):\n",
      " #   Column                    Non-Null Count  Dtype \n",
      "---  ------                    --------------  ----- \n",
      " 0   class                     8124 non-null   object\n",
      " 1   cap-shape                 8124 non-null   object\n",
      " 2   cap-surface               8124 non-null   object\n",
      " 3   cap-color                 8124 non-null   object\n",
      " 4   bruises                   8124 non-null   object\n",
      " 5   odor                      8124 non-null   object\n",
      " 6   gill-attachment           8124 non-null   object\n",
      " 7   gill-spacing              8124 non-null   object\n",
      " 8   gill-size                 8124 non-null   object\n",
      " 9   gill-color                8124 non-null   object\n",
      " 10  stalk-shape               8124 non-null   object\n",
      " 11  stalk-root                8124 non-null   object\n",
      " 12  stalk-surface-above-ring  8124 non-null   object\n",
      " 13  stalk-surface-below-ring  8124 non-null   object\n",
      " 14  stalk-color-above-ring    8124 non-null   object\n",
      " 15  stalk-color-below-ring    8124 non-null   object\n",
      " 16  veil-type                 8124 non-null   object\n",
      " 17  veil-color                8124 non-null   object\n",
      " 18  ring-number               8124 non-null   object\n",
      " 19  ring-type                 8124 non-null   object\n",
      " 20  spore-print-color         8124 non-null   object\n",
      " 21  population                8124 non-null   object\n",
      " 22  habitat                   8124 non-null   object\n",
      "dtypes: object(23)\n",
      "memory usage: 1.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.head()\n",
    "#df.discribe()\n",
    "df.info()\n",
    "\n",
    "# Hier können wir überlegen was für von JP mitnehmen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e8e9fe",
   "metadata": {},
   "source": [
    "**4. Preparing the data**\n",
    "\n",
    "General guidelines: \n",
    "- Work on copies of the data (keep the original dataset intact). \n",
    "- Write functions for all data transformations you apply. \n",
    "    Reasons:\n",
    "        - Reproducability: to easily prepare new dataset or for future projects\n",
    "        - To clean and prepare the test set\n",
    "        - To clean and prepare new data instances\n",
    "        - To make it easy to treat your preparation choices as hyperparameters \n",
    "- Data cleaning:\n",
    "    - Fix or remove outliers (optional).\n",
    "    - Fill in missing values (e.g., with zero, mean, median...) or drop their rows (or columns).\n",
    "- Feature selection (optional): Drop the attributes that provide no useful information for the task. \n",
    "- Feature engineering, where appropriates:\n",
    "    - Discretize continuous features.\n",
    "    - Decompose features (e.g., categorical, date/time, etc.).\n",
    "    - Add promising transformations of features (e.g., log(x), sqrt(x), x^2).\n",
    "    - Aggregate features into promising new features. \n",
    "- Feature scaling: standardize or normalize features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53460d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#XGBoost\n",
    "le = LabelEncoder()\n",
    "for column in df.columns:\n",
    "df[column]= le.fit_transform(df[column])\n",
    "x = df.drop('class',axis=1)\n",
    "y = df['class']\n",
    "X_train,X_test,y_train,y_test = train_test_split(x,y,train_size = 0.8, random_state=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa241cd4",
   "metadata": {},
   "source": [
    "**5. Selecting and training the model**\n",
    "\n",
    "1. If the data is huge, sample smaller training sets so you can train many different models in a reasonable time. \n",
    "2. Try to automate the process as much as possible. \n",
    "3. Train many quick models from different categories (e.g., linear, naive Bayes, SVM, Rand. Forests, neural net, etc.) using standard parameters. \n",
    "4. Measure and compare performance: For each model, use N-fold cross-validation and compute mean and standard deviation of performance.\n",
    "5. Analyze the most significant attributes/features for each algorithm. \n",
    "6. Analyze the types of errors the models make: What data would a human have used to avoid these errors? \n",
    "7. Have a quick round of feature selection and feature engineering. \n",
    "8. Have one or two more quick iterations of the five previous steps. \n",
    "9. Short-list the top three to five most promising models, preferring models that make different types of errors.\n",
    "\n",
    "\n",
    "Models relevant for this project: `Logistic Regression`, `XGBoost` and `ThirdModelToBeChosen` (maybe a DecisionTree?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2252fa65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "model = xgb.XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    max_depth=4,\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad784c7",
   "metadata": {},
   "source": [
    "**6. Fine-tuning the model**\n",
    "\n",
    "General guidelines: \n",
    "1. Use as much data as possible for this step. \n",
    "2. As always automate what you can. \n",
    "3. Fine-tune the hyperparameters using cross-validation. \n",
    "4. Treat your data transformation choices as hyperparameters (e.g. replace missing values with zero or median? Or just drop the rows?). \n",
    "5. Unless there are very few hyperparamter values to explore, prefer random search over grid search. \n",
    "6. Try Ensemble methods. Combining your best models will often perform better than running them invdividually. \n",
    "7. Once you are confident about your final model, measure its performance on the test set to estimate the generalization error.\n",
    "8. Don't tweak your model after measuring the generalization error: you would just start overfitting the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f8ee38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cade92a5",
   "metadata": {},
   "source": [
    "**7. Present your solution**\n",
    "\n",
    "General guidelines: \n",
    "- Document what you have done. \n",
    "- Create a nice presentation: Make sure you highlight the big picture first. \n",
    "- Explain why your solution achieves the business objective. \n",
    "- Don't forget to present interesting points you noticed along the way. Describe what worked and what did not. List your assumptions and your system's limitations. \n",
    "- Ensure your key findings are communicated through beautiful visualizations or easy-toremember statements (e.g., \"the median income is the number-one predictor of housing prices\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4d06f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "41b0891f",
   "metadata": {},
   "source": [
    "**8. Launch, monitor and maintain**\n",
    "\n",
    "General guidelines: \n",
    "- Get your solution ready for production (plug into production data inputs, write unit tests, etc.). \n",
    "- Write monitoring code to check your system's live performance at regular intervals and trigger alerts when it drops. Beware of slow degradation too: models tend to degrade as data evolves. Measuring performance may require a human pipeline (e.g., via a crowdsourcing service). Also monitor your inputs' quality (e.g., a malfunctioning sensor sending random values). This is particulary important for online learning systems. \n",
    "- Retrain your models on a regular basis on fresh data (automate as much as possible)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
